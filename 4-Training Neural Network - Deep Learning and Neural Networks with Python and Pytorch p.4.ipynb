{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Network - Deep Learning and Neural Networks with Python and Pytorch p.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Neural Network\n",
    "In the previous tutorial, we created the code for our neural network. In this deep learning with Python and Pytorch tutorial, we'll be actually training this neural network by learning how to iterate over our data, pass to the model, calculate loss from the result, and then do backpropagation to slowly fit our model to the data.\n",
    "\n",
    "Code up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for us, the \"data\" that we're using from Pytorch is actually nice fancy object that is making life easy for us at the moment. It's already in pretty batches and we just need to iterate over it. Next, we want to calculate loss and specify our optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### side note from *Andy*\n",
    "Other Pytorch loss functions can be found at [Pytorch.nn](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "Other Pytorch optimizers can be found at [Class Optimizer](https://pytorch.org/cppdocs/api/classtorch_1_1optim_1_1_optimizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ur loss_function is what calculates \"how far off\" our classifications are from reality. As humans, we tend to think of things in terms of either right, or wrong. With a neural network, and arguably humans too, our accuracy is actually some sort of scaling score.\n",
    "\n",
    "For example, you might be highly confident that something is the case, but you are wrong. Compare this to a time when you really aren't certain either way, but maybe think something, but are wrong. In these cases, the degree to which you're wrong doesn't matter in terms of the choice necessarily, but in terms of you learning, it does.\n",
    "\n",
    "In terms of a machine learning by tweaking lots of little parameters to slowly get closer and closer to fitting, it definitely matters how wrong things are.\n",
    "\n",
    "For this, we use loss, which is a measurement of how far off the neural network is from the targeted output. There are a few types of loss calculations. A popular one is mean squared error, but we're trying to use these scalar-valued classes.\n",
    "\n",
    "In general, you're going to have two types of classes. One will just be a scalar value, the other is what's called a one_hot array/vector.\n",
    "\n",
    "In our case, a zero might be classified as:\n",
    "\n",
    "0 or [1, 0, 0, 0, 0, 0, 0 ,0 ,0 ,0]\n",
    "\n",
    "[1, 0, 0, 0, 0, 0, 0 ,0 ,0 ,0] is a one_hot array where quite literally one element only is a 1 and the rest are zero. The index that is hot is the classification.\n",
    "\n",
    "A one_hot vector for a a 3 would be:\n",
    "\n",
    "[0, 0, 0, 1, 0, 0, 0 ,0 ,0 ,0]\n",
    "\n",
    "I tend to use one_hot, but this data is specifying a scalar class, so 0, or 1, or 2...and so on.\n",
    "\n",
    "Depending on what your targets look like, you will need a specific loss.\n",
    "\n",
    "For one_hot vectors, I tend to use mean squared error.\n",
    "\n",
    "For these scalar classifications, I use cross entropy.\n",
    "\n",
    "Next, we have our optimizer. This is the thing that adjusts our model's adjustable parameters like the weights, to slowly, over time, fit our data. I am going to have us using Adam, which is Adaptive Momentum. This is the standard go-to optimizer usually. There's a new one called rectified adam that is gaining steam. I haven't had the chance yet to make use of that in any project, and I do not think it's available as just an importable function in Pytorch yet, but keep your eyes peeled for it! For now, Adam will do just fine I'm sure. The other thing here is lr, which is the learning rate. A good number to start with here is 0.001 or 1e-3. The learning rate dictates the magnitude of changes that the optimizer can make at a time. Thus, the larger the LR, the quicker the model can learn, but also you might find that the steps you allow the optimizer to make are actually too big and the optimizer gets stuck bouncing around rather than improving. Too small, and the model can take much longer to learn as well as also possibly getting stuck.\n",
    "\n",
    "Imagine the learning rate as the \"size of steps\" that the optimizer can take as it searches for the bottom of a mountain, where the path to the bottom isn't necessarily a simple straight path down. Here's some lovely imagery to help explain learning rate:\n",
    "(learning-rate-too-high-too-low)[https://pythonprogramming.net/static/images/machine-learning/learning-rate-too-high-too-low.png]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black line is the \"path\" to the bottom of the optimization curve. When it comes to optimizing, sometimes you have to get worse in order to actually get beyond some local optimum. The optimizer doesn't know what the absolute best spot could be, it just takes steps to see if it can find it. Thus, as you can see in the image, if the steps are too big, it will never get to the lower points. If the steps are too small (learning rate too small), it can get stuck as well long before it reaches a bottom. The goal is for something more like:(learning-rate-just-right)[https://pythonprogramming.net/static/images/machine-learning/learning-rate-just-right.png]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simpler tasks, a learning rate of 0.001 usually is more than fine. For more complex tasks, you will see a learning rate with what's called a decay. Basically you start the learning rate at something like 0.001, or 0.01...etc, and then over time, that learning rate gets smaller and smaller. The idea being you can initially train fast, and slowly take smaller steps, hopefully getthing the best of both worlds:(decaying-lr)[https://pythonprogramming.net/static/images/machine-learning/decaying-lr.png]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on learning rates and decay later. For now, 0.001 will work just fine, and you just need to think of learning rate as what it sounds like: How quickly should we try to get this optimizer to optimize things.\n",
    "\n",
    "Now we can iterate over our data. In general, you will make more than just 1 pass through your entire training dataset.\n",
    "\n",
    "Each full pass through your dataset is referred to as an epoch. In general, you will probably have somewhere between 3 and 10 epochs, but there's no hard rule here.\n",
    "\n",
    "Too few epochs, and your model wont learn everything it could have.\n",
    "\n",
    "Too many epochs and your model will over fit to your in-sample data (basically memorize the in-sample data, and perform poorly on out of sample data).\n",
    "\n",
    "Let's go with 3 epochs for now. So we will loop over epochs, and each epoch will loop over our data. Something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0061, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#running this on a regular CPU\n",
    "for epoch in range(3): # 3 full passes over the data\n",
    "    for data in trainset:  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every line here is commented, but the concept of gradients might not be clear. Once we pass data through our neural network, getting an output, we can compare that output to the desired output. With this, we can compute the gradients for each parameter, which our optimizer (Adam, SGD...etc) uses as information for updating weights.\n",
    "\n",
    "This is why it's important to do a net.zero_grad() for every step, otherwise these gradients will add up for every pass, and then we'll be re-optimizing for previous gradients that we already optimized for. There could be times when you intend to have the gradients sum per pass, like maybe you have a batch of 10, but you want to optimize per 50 or something. I don't think people really do that, but the idea of Pytorch is to let you do whatever you want.\n",
    "\n",
    "So, for each epoch, and for each batch in our dataset, what do we do?\n",
    "\n",
    "Grab the features (X) and labels (y) from current batch\n",
    "Zero the gradients (net.zero_grad)\n",
    "Pass the data through the network\n",
    "Calculate the loss\n",
    "Adjust weights in the network with the hopes of decreasing loss\n",
    "As we iterate, we get loss, which is an important metric, but we care about accuracy. So, how did we do? To test this, all we need to do is iterate over our test set, measuring for correctness by comparing output to target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.966\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        #print(output)\n",
    "        for idx, i in enumerate(output):\n",
    "            #print(torch.argmax(i), y[idx])\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, I would say we did alright there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[0].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above might be slightly confusing. I'll break it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6936e+01, -7.4109e+00, -7.4424e+00, -4.0751e+00, -1.5709e+01,\n",
      "        -1.3095e+01, -2.6675e+01, -1.8489e-02, -1.0788e+01, -9.0661e+00],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "a_featureset = X[0]\n",
    "reshaped_for_network = a_featureset.view(-1,784) # 784 b/c 28*28 image resolution.\n",
    "output = net(reshaped_for_network) #output will be a list of network predictions.\n",
    "first_pred = output[0]\n",
    "print(first_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which index value is the greatest? We use argmax to find this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "biggest_index = torch.argmax(first_pred)\n",
    "print(biggest_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's so much more we could do here with this example, but I think it's best we move on.\n",
    "\n",
    "More things to consider: Tracking/graphing loss and accuracy over time, comparing in and out of sample accuracy, maybe hand-drawing our own example to see if it works...etc.\n",
    "\n",
    "As fun and easy as it is to use a pre-made dataset, one of the first things you really want to do once you learn deep learning is actually do something you're interested in, which often means your own dataset that isn't prepared for us like this one was.\n",
    "\n",
    "In the coming tutorials, we're going to discuss the convolutional neural network and classify images of dogs and cats, starting from the raw images, as well as covering some of the other topics like tracking training over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### side note from *Andy*\n",
    "If you liked this series there are 4 more modules talking about CNN.  Let me know if you would like this in a jupyter notebook, and I can convert it over for you.\n",
    "Check out [https://pythonprogramming.net](https://pythonprogramming.net)\n",
    "\n",
    "If you would like to see how Jeremy approached this, check it out.\n",
    "However, be careful as they move to CNN vs ANN.\n",
    "[WHAT IS TORCH.NN REALLY?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
